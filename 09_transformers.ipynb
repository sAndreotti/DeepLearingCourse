{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Exercise 5 : Neural machine Translation with `nn.Transformer` and Multi30k dataset\n",
    "========================================================\n",
    "\n",
    "**USI, Deep Learning lab SA 2024-2025**\n",
    "\n",
    "Lecturer: Eleonora Vercesi. TAs: Alvise Dei Rossi, Stefano Huber, Gabriele Dominici\n",
    "\n",
    "========================================================\n",
    "\n",
    "In this exercise we're going to tackle a \"simple\" (toy) machine translation task with a full transformer model, implemented with Pytorch.\n",
    "We will use the [Multi30k dataset from torchtext\n",
    "library](https://pytorch.org/text/stable/datasets.html#multi30k) that\n",
    "yields a pair of source-target raw sentences (source: German, target: English). Originally this dataset was introduced by researchers to stimulate multilingual multimodal research (sentences are image descriptions, [link to article](https://arxiv.org/abs/1605.00459)). It includes approximately 30 thousand sentence-pairs, hence the name.\n",
    "\n",
    "In this example, we show how to load the dataset, tokenize raw text sentences,\n",
    "build vocabulary, and numericalize tokens into tensor. We'll then built the transformer model and train it onto the processed data.\n",
    "\n",
    "\n",
    "\n",
    "For comparison the original Trasformer model (which is not too far with respect to the implementation we'll see today) discussed in [AIAYN](https://user.phil.hhu.de/~cwurm/wp-content/uploads/2020/01/7181-attention-is-all-you-need.pdf) was instead trained on the WMT 2014 English-German dataset consisting of 4.5M sentence pairs, with 37k tokens and on 8 NVIDIA P100 GPUs for 12 hours."
   ],
   "metadata": {
    "id": "vTJclL94jbW1"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Libraries version & Tokenizer download\n",
    "===================="
   ],
   "metadata": {
    "id": "pASTPAuooV-r"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Note**: You **SHOULD definitely** use a GPU for this exercise; we'll use Colab GPUs but if you finished your GPU time on Colab, you can also run this notebook on Kaggle or Lightning.\n",
    "\n",
    "The following versioning configuration will throw an error but this shouldn't be relevant for running the rest of the notebook. Keep it unchanged."
   ],
   "metadata": {
    "id": "Ml5y3WPEsD20"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install torch==2.0.0 torchdata==0.6.0 spacy==3.7.2 numpy==1.24.4 torchvision==0.15.1 torchtext==0.15.1 portalocker>=2.0.0"
   ],
   "metadata": {
    "id": "c4bWf0DPLkfu",
    "ExecuteTime": {
     "end_time": "2024-11-18T14:35:57.358078Z",
     "start_time": "2024-11-18T14:35:57.232526Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: 2.0.0 not found\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this notebook we'll download the tokenizer, focusing more on the architecture of the model; but of course you could tokenize the sentences with custom tokenizers. Note that the ``torchtext`` tokenizers are a bit lacking when it comes to multi-language support. Instead we'll use a tokenizer from ``SpaCy`` ([link](https://spacy.io/)), which originally provides strong support for tokenization in languages other than English. Another popular choice are HuggingFace [tokenizers](https://github.com/huggingface/tokenizers)."
   ],
   "metadata": {
    "id": "StUb8lYOtZdk"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download de_core_news_sm"
   ],
   "metadata": {
    "id": "Kj5rw1SQtMDL",
    "ExecuteTime": {
     "end_time": "2024-11-18T15:03:02.732293Z",
     "start_time": "2024-11-18T15:03:02.453838Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/stefano/miniconda3/envs/DeepL/bin/python: No module named spacy\r\n",
      "/Users/stefano/miniconda3/envs/DeepL/bin/python: No module named spacy\r\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data Downloading and Processing\n",
    "============================"
   ],
   "metadata": {
    "id": "ITYDKys4o5XI"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JtRdeMZMKhCZ",
    "ExecuteTime": {
     "end_time": "2024-11-18T14:35:58.410650Z",
     "start_time": "2024-11-18T14:35:57.658464Z"
    }
   },
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import multi30k, Multi30k, IWSLT2017\n",
    "from typing import Iterable, List\n",
    "\n",
    "\n",
    "# Note that the links to the original dataset from torchtext are broken\n",
    "# we need instead to load the dataset from zip files from a temporary repo\n",
    "# Refer to https://github.com/pytorch/text/issues/1756#issuecomment-1163664163 for more info\n",
    "multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
    "multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
    "\n",
    "SRC_LANGUAGE = 'de'\n",
    "TGT_LANGUAGE = 'en'\n",
    "\n",
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stefano/miniconda3/envs/DeepL/lib/python3.12/site-packages/torchtext/data/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/Users/stefano/miniconda3/envs/DeepL/lib/python3.12/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/Users/stefano/miniconda3/envs/DeepL/lib/python3.12/site-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/Users/stefano/miniconda3/envs/DeepL/lib/python3.12/site-packages/torchtext/datasets/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "We're going first to get the tokenizers and [vocabularies](https://pytorch.org/text/main/vocab.html)."
   ],
   "metadata": {
    "id": "sNsRMNzsueZQ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Keep in mind that data samples in the dataset are tuples of sentences like (sentence_in_german, sentence_in_english)"
   ],
   "metadata": {
    "id": "A9EJ3FSeYqaS"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4xSHCIhLKhCa",
    "ExecuteTime": {
     "end_time": "2024-11-18T15:05:38.503707Z",
     "start_time": "2024-11-18T15:05:38.482472Z"
    }
   },
   "source": [
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "\n",
    "# helper function to yield list of tokens\n",
    "def yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n",
    "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
    "\n",
    "    for data_sample in data_iter:\n",
    "        yield token_transform[language](data_sample[language_index[language]])\n",
    "\n",
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "# unk -> tokens not in the vocabulary\n",
    "# pad -> padding - tokens to fill the sequence to the max length\n",
    "# bos -> begin of sentence\n",
    "# eos -> end of sentence\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    # Training data Iterator\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    # Create torchtext's Vocab object\n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n",
    "                                                    min_freq=1,\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True)\n",
    "\n",
    "# Set ``UNK_IDX`` as the default index. This index is returned when the token is not found.\n",
    "# If not set, it throws ``RuntimeError`` when the queried token is not found in the Vocabulary.\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "  vocab_transform[ln].set_default_index(UNK_IDX)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please install SpaCy. See the docs at https://spacy.io for more information.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m token_transform[SRC_LANGUAGE] \u001B[38;5;241m=\u001B[39m get_tokenizer(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspacy\u001B[39m\u001B[38;5;124m'\u001B[39m, language\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mde_core_news_sm\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      2\u001B[0m token_transform[TGT_LANGUAGE] \u001B[38;5;241m=\u001B[39m get_tokenizer(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspacy\u001B[39m\u001B[38;5;124m'\u001B[39m, language\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124men_core_web_sm\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# helper function to yield list of tokens\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/DeepL/lib/python3.12/site-packages/torchtext/data/utils.py:91\u001B[0m, in \u001B[0;36mget_tokenizer\u001B[0;34m(tokenizer, language)\u001B[0m\n\u001B[1;32m     89\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tokenizer \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspacy\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m     90\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 91\u001B[0m         \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mspacy\u001B[39;00m\n\u001B[1;32m     93\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     94\u001B[0m             spacy \u001B[38;5;241m=\u001B[39m spacy\u001B[38;5;241m.\u001B[39mload(language)\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'spacy'"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's make a couple examples:"
   ],
   "metadata": {
    "id": "9FRVT2Syt__Y"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "en_vocab = vocab_transform[TGT_LANGUAGE]\n",
    "de_vocab = vocab_transform[SRC_LANGUAGE]\n",
    "print(f\"English vocab size: {len(en_vocab)}\")\n",
    "print(f\"German vocab size: {len(de_vocab)}\")\n",
    "\n",
    "# Tokenization of red car in english and german\n",
    "print(en_vocab[\"red\"], en_vocab[\"car\"], en_vocab[\"Car\"])\n",
    "print(de_vocab[\"rotes\"], de_vocab[\"Auto\"], de_vocab[\"auto\"])"
   ],
   "metadata": {
    "id": "L7v8tjUbt-jR"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fpTngYZHKhCa"
   },
   "source": [
    "Encoder Decoder Transformer Network\n",
    "=================================\n",
    "\n",
    "Transformer is a [sequence to sequence](https://proceedings.neurips.cc/paper/2014/hash/a14ac55a4f27472c5d894ec1c3c743d2-Abstract.html) model introduced in [\"Attention is all you\n",
    "need\"](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)\n",
    "paper for machine translation tasks.\n",
    "\n",
    "<img src=\"https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png\" width=400 height=500>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Embedding and positional encoding\n",
    "\n",
    "Below, we will create a\n",
    "Seq2Seq network that uses Transformer. The network consists of three\n",
    "parts. First part is the embedding layer. This layer converts tensor of\n",
    "input indices into corresponding tensor of input embeddings. These\n",
    "embedding are further augmented with positional encodings to provide\n",
    "position information of input tokens to the model (more details [here](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)).\n",
    "\n",
    "<img src=\"https://jalammar.github.io/images/t/transformer_positional_encoding_example.png\" width=800 height=300>\n",
    "\n",
    "In the following image Depth is the embedding size.\n",
    "\n",
    "<img src=\"https://kazemnejad.com/img/transformer_architecture_positional_encoding/positional_encoding.png\" width=800 height=350>\n",
    "\n",
    "We report here the original implementation\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "Try to implement a Pytorch layer following the equations above. Consider that it must match the dimension of the Embedding Layer reported below.\n"
   ],
   "metadata": {
    "id": "wYmy_GcMQ9QJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "import math\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    pass # TODO\n",
    "\n",
    "\n",
    "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)"
   ],
   "metadata": {
    "id": "HxuLdLhuRYuJ"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Solution"
   ],
   "metadata": {
    "id": "EqK1GLFLRO12"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QK85KjcKKhCa"
   },
   "source": [
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "import math\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # For a better explanation for the following computation, refer to the link:\n",
    "        # https://ai.stackexchange.com/questions/41670/why-use-exponential-and-log-in-positional-encoding-of-transformer\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "\n",
    "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Encoder and decoder"
   ],
   "metadata": {
    "id": "98DxpAzJRSAI"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Embedding layers and positional encoding are used both in the Encoder and Decoder part of the Transformer architecture.\n",
    "\n",
    "The key difference wrt recurrent networks is the attention mechanism, also present in both encoder and decoder.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*ArTXQZip_TwbU6gLshXOEw.png\">\n",
    "\n",
    "For each token we create a Query vector $q$ a key vector $k$ and a value vector $v$, through linear transformation. Stacking these vectors we obtain the corresponding matrices $Q$, $K$, $V$.\n",
    "\n",
    "Self attention is then computed as:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q,K,V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{\\text{d}_{k}}}\\right) V\n",
    "$$\n",
    "\n",
    "The $QK^T$ matrix multiplication computes the relevance between each pair of words (attention weights).\n",
    "\n",
    "This relevance is then used as a \"factor\" to compute the weighted sum of all the values words.\n",
    "\n",
    "This is done in parallel multiple times, for multiple heads (`nhead`). The idea is to capture different aspects of the input.\n",
    "\n",
    "Self-attention is present both in the encoder and decoder parts of the network, with some key differences:\n",
    "\n",
    "- For the encoder self-attention the input sequence pays attention to itself: masking is done only on PAD tokens, to avoid attention weight on padding.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*026nzf4bw_DdSZ7kLJWcog.png\">\n",
    "\n",
    "- For the decoder, multi head attention is present twice:\n",
    "  - Right after positional encoding, we have an attention mechanism where the the output sequence pays attention to itself. The mask is defined so that it will prevent the model from looking into future words when making predictions (masked self-attention). Padding is also masked. Said in other words, in the Decoder Self-attention masking serves to prevent the decoder from ‘peeking’ ahead at the rest of the target sentence when predicting the next word.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*cawtdZLjT9hp7ByG2vcKOQ.png\">\n",
    "\n",
    "  - - After the previous attention mechanism, the Q matrix arrives from it, while K and V from the Encoder. In practice, In the Encoder-Decoder Attention, the Query is obtained from the target sentence and the Key/Value from the source sentence. Thus it computes the relevance of each word in the target sentence to each word in the source sentence.\n",
    "\n",
    "\n",
    "  The masking is done as per the encoder part here, being careful about padding.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*OYi49Pkg-Vl3D4HleEuu7g.png\">\n",
    "\n",
    "\n",
    "**Note**: not in images, but an encoded \\<EOS\\> token would be present at the end of each sequence in the encoder, while an encoded \\<BOS\\> token is present in the decoder at the start of each sequence.\n",
    "\n",
    "Keep in mind that the decoder part of the Transformers works differently when in training (teacher-forcing) and when in inference mode (autoregressive)\n",
    "\n",
    "\n",
    "\n",
    "This process is then followed by a normalization layer, skip connections, feedforward and again normalization layer + skip connection.\n",
    "\n",
    "Multiple encoder (`num_encoder_layers`) and decoder (`num_decoder_layers`) layers are stacked. Note that dimension of the representation of the sequence is preserved throught the architecture.\n",
    "Finally, the output of the Transformer model is passed through\n",
    "a linear layer that gives unnormalized probabilities for each token in the\n",
    "target language.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*dQTK3oeYqOBUDVgNktSpCw.png\" height=600 width=750>\n",
    "\n",
    "the [Transformer](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html) model is easily implemented with its Pytorch class."
   ],
   "metadata": {
    "id": "aZ4Dy5jevDHH"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.transformer = Transformer(d_model=emb_size,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            emb_size, dropout=dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer.encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer.decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)"
   ],
   "metadata": {
    "id": "5Q0iX6omvAwV"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FSpN-puLKhCa"
   },
   "source": [
    "We will also define functions for the masks:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dcsa1XzxKhCb"
   },
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    ## Decoder mask\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n",
    "\n",
    "# Just to have an idea of what it looks like:\n",
    "print(generate_square_subsequent_mask(5))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZkPwEpIKhCb"
   },
   "source": [
    "Let\\'s now define the parameters of our model and instantiate the same.\n",
    "Below, we also define our loss function which is the cross-entropy loss\n",
    "and the optimizer used for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QwvGEqg4KhCb"
   },
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "BATCH_SIZE = 128\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define the loss function and the optimizer. Remember, the loss should ignore padding; take a look at the Pytorch documentation of the appropriate loss (which one should you use for multi-class classificaiton?) in order to understand how to do it.\n",
    "\n",
    "Also, how would you check if your model is too large or if everything you expect is within the model?"
   ],
   "metadata": {
    "id": "KcLAEwMM15gL"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "loss_fn = # TODO\n",
    "\n",
    "optimizer = # TODO\n",
    "\n",
    "# TODO"
   ],
   "metadata": {
    "id": "1qyxBr1X145q"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Solution"
   ],
   "metadata": {
    "id": "qVbaTo-r2SDT"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Cross entropy is not computed for the padded part of the sentence.\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ],
   "metadata": {
    "id": "0dnbiJSO2RYG"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "I always find interesting to keep track of the number of trainable parameters when I'm deciding the hyperparameters of a model:"
   ],
   "metadata": {
    "id": "rX1EMvHjB7qs"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "count_parameters(transformer)"
   ],
   "metadata": {
    "id": "93alOCSrBmXj"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Instead if you want a more complete overview of the model:"
   ],
   "metadata": {
    "id": "Dkg4LvGWTXqz"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(transformer)"
   ],
   "metadata": {
    "id": "8Zj2-AMzCJB_"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oDPYAvUtKhCb"
   },
   "source": [
    "Collation\n",
    "=========\n",
    "\n",
    "As seen in the `Data Sourcing and Processing` section, our data iterator\n",
    "yields a pair of raw strings. We need to convert these string pairs into\n",
    "the batched tensors that can be processed by our `Seq2Seq` network\n",
    "defined previously. Below we define our collate function that converts a\n",
    "batch of raw strings into batch tensors that can be fed directly into\n",
    "our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XLQrXHa3KhCb"
   },
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids: List[int]):\n",
    "    return torch.cat((torch.tensor([BOS_IDX]),\n",
    "                      torch.tensor(token_ids),\n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# ``src`` and ``tgt`` language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Given the transform helper functions above, how would you define a collate function, which will be given to the DataLoaders, to appropriately prepare batches? Remember to also pad sequences with the ``pad_sequence`` function, imported from torch utils."
   ],
   "metadata": {
    "id": "Ux7GuRXv2yi9"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def collate_fn(batch):\n",
    "  pass # TODO"
   ],
   "metadata": {
    "id": "j_8B6HnU3Ipd"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Solution"
   ],
   "metadata": {
    "id": "pS9EPDnI3MRS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# function to collate data samples into batch tensors\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    # print(\"batch: \", batch)\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
    "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    return src_batch, tgt_batch"
   ],
   "metadata": {
    "id": "x5RtsXMi2xx9"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training and inference functions"
   ],
   "metadata": {
    "id": "OdUkQVC2CzAt"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K079UrMSKhCc"
   },
   "source": [
    "Let\\'s define training and evaluation loop that will be called for each\n",
    "epoch.\n",
    "\n",
    "Note that in the validation epoch we also compute [perplexity](https://medium.com/nlplanet/two-minutes-nlp-perplexity-explained-with-simple-probabilities-6cdc46884584).\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8zd3guSWKhCc"
   },
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def train_epoch(model, optimizer):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in train_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(list(train_dataloader))\n",
    "\n",
    "\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in val_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        losses += loss.item()\n",
    "\n",
    "        perplexity = math.exp(loss.item())\n",
    "\n",
    "    return losses / len(list(val_dataloader)), perplexity\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's also write a couple functions to greedily decode sequences produced by the model."
   ],
   "metadata": {
    "id": "IgGe3597qGRg"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                    .type(torch.bool)).to(DEVICE)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "    return ys\n",
    "\n",
    "\n",
    "def translate(model: torch.nn.Module, src_sentence: str):\n",
    "    model.eval()\n",
    "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
    "    num_tokens = src.shape[0]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = greedy_decode(\n",
    "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
    "    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
   ],
   "metadata": {
    "id": "zREZRgNZp8gj"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Write a function to qualitatively evaluate the quality of the translation of a single sentence, from the validation set, from the model. This is going to be used at the end of every training epoch to have a general idea of how / if the model is improving. Use the functions defined above within it."
   ],
   "metadata": {
    "id": "Kjk9Wbqk33tj"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def evaluate_single_sentence():\n",
    "  pass #TODO"
   ],
   "metadata": {
    "id": "_7ONsGYF4YoD"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Solution"
   ],
   "metadata": {
    "id": "NI78Bpme4ghc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def evaluate_single_sentence(model, iteration):\n",
    "    model.eval()\n",
    "\n",
    "    val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
    "    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    src, tgt = next(iter(val_dataloader))\n",
    "    src_sample = src[:, iteration]\n",
    "    tgt_sample = tgt[:, iteration]\n",
    "    src_str = \" \".join([de_vocab.lookup_token(idx) for idx in list(src_sample) if idx not in [1,2,3]] )\n",
    "    tgt_str = \" \".join([en_vocab.lookup_token(idx) for idx in list(tgt_sample) if idx not in [1,2,3]] )\n",
    "\n",
    "    return src_str, tgt_str, translate(model, src_str)"
   ],
   "metadata": {
    "id": "UwEdyFEd326w"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model training"
   ],
   "metadata": {
    "id": "tCcywK3w4pHP"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GTyJx9VpKhCc"
   },
   "source": [
    "We finally have everything we need to train our Transformer model.\n",
    "\n",
    "Using all the functions defined above, train the transformer model for 20 epochs, keep track of training and validation losses at every epoch, as they will be useful to evaluate the model afterwards.\n",
    "\n",
    "Make sure to print out essential information during the training.\n",
    "For each epoch you should print out:\n",
    "\n",
    "- Epoch number\n",
    "- Training loss\n",
    "- Validation loss\n",
    "- Perplexity\n",
    "- A single source sentence\n",
    "- The corresponding target sentence\n",
    "- The translation of the model for that sentence\n",
    "- Optionally the amount of time for the epoch\n",
    "\n",
    "\n",
    "The training should take approximately 15-20 minutes (with Colab's GPU).\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from timeit import default_timer as timer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "training_losses = # TODO\n",
    "val_losses = # TODO\n",
    "\n",
    "print(\"Untrained model\")\n",
    "source, target, translation = evaluate_single_sentence(transformer, 0)\n",
    "\n",
    "print(\"*\"*20)\n",
    "print(\"Training\")\n",
    "print(\"*\"*20)\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "\n",
    "\n",
    "    # TODO\n",
    "\n",
    "\n",
    "    end_time = timer()"
   ],
   "metadata": {
    "id": "4zWk56V45vNN"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Solution"
   ],
   "metadata": {
    "id": "DqyWcu97571S"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MyZ08G_PKhCc"
   },
   "source": [
    "from timeit import default_timer as timer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "training_losses = []\n",
    "val_losses = []\n",
    "\n",
    "print(\"Untrained model\")\n",
    "source, target, translation = evaluate_single_sentence(transformer, 0)\n",
    "print(f\"Source: {source}\")\n",
    "print(f\"Target: {target}\")\n",
    "print(f\"Translation: {translation}\")\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"*\"*20)\n",
    "print(\"Training\")\n",
    "print(\"*\"*20)\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer)\n",
    "    end_time = timer()\n",
    "    val_loss, perplexity = evaluate(transformer)\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    print(f\"Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, Perplexity: {perplexity}\")\n",
    "    print(f\"Epoch time = {(end_time - start_time):.3f}s\")\n",
    "    training_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    source, target, translation = evaluate_single_sentence(transformer, epoch)\n",
    "\n",
    "    print(f\"Source: {source}\")\n",
    "    print(f\"Target: {target}\")\n",
    "    print(f\"Translation: {translation}\")\n",
    "    print(\"\\n\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model evaluation"
   ],
   "metadata": {
    "id": "8dhZ_JVlC3j0"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As usual let's plot out the evolution of the training and validation losses"
   ],
   "metadata": {
    "id": "r2-W96Pl5_4K"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TODO"
   ],
   "metadata": {
    "id": "KBeRpdrW7ZMj"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Solution"
   ],
   "metadata": {
    "id": "NIW6DvgT7bIl"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(1,NUM_EPOCHS+1),training_losses, label=\"Training loss\")\n",
    "plt.plot(range(1,NUM_EPOCHS+1),val_losses, label=\"Validation loss\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.5)\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "-8U-fEKm3hYY"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test translation"
   ],
   "metadata": {
    "id": "NC-oxu4G7ekz"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's test the translation with a few sentences, different conditions:"
   ],
   "metadata": {
    "id": "XxaiUJFMEC9g"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "53AQPVl7KhCd"
   },
   "source": [
    "# For a sentence we know it's in the dataset\n",
    "print(\"In dataset\")\n",
    "print(translate(transformer, \"Eine Gruppe von Menschen steht vor einem Iglu .\")) # A group of people stands in front of an igloo.\n",
    "# For some sentences that are appropriate for the dataset type (image captions)\n",
    "print(\"In appropriate context\")\n",
    "print(translate(transformer, \"Ein rotes Auto rast auf der Autobahn\")) # A red car is racing on the highway\n",
    "print(translate(transformer, \"Ein kleiner Junge, der mit einem Ball spielt\")) # A little boy playing with a ball\n",
    "# Something completely different, a statement\n",
    "print(\"Out of domain\")\n",
    "print(translate(transformer, \"Der Präsident des Verbandes kündigte eine Konferenz an\")) # The president of the association announced a conference\n",
    "# Something that could be from a messaging app\n",
    "print(translate(transformer, \"Wir sehen uns morgen um 18 Uhr in der Nähe des städtischen Schwimmbades\")) # See you tomorrow at 6 p.m. near the municipal swimming pool"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can try also with your own test sentences:"
   ],
   "metadata": {
    "id": "dsC6j4fs6MGq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "my_sentence = # TODO (note that this should be in German)\n",
    "print(translate(transformer, my_sentence))"
   ],
   "metadata": {
    "id": "quXRAo3S6Kfp"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We've seen a lot in this exercise, but as you can see the model is limited. After all you wouldn't expect to solve machine translation on a laptop in 30 minutes, right?\n",
    "If you want you can try several things:\n",
    "- change minumum number of times a word has to appear in the training set to be included in the vocabulary\n",
    "- change tokenization sm-md-lg\n",
    "- change tokenization completely\n",
    "- change hyperparameters of the transformer architecture (**emb size**, **ffnn_dim**, n_enc, n_dec, ...)\n",
    "- change training hyperparameters (batch size, number of epochs)\n",
    "- implement other evaluation metrics (e.g. [Beau score](https://))\n",
    "- implement a different way to generate sequences in inference (e.g.[ Beam search](https://towardsdatascience.com/foundations-of-nlp-explained-visually-beam-search-how-it-works-1586b9849a24))\n",
    "- You can also use this notebook as a starting point for a different machine translation dataset / task, the steps are approximately the same."
   ],
   "metadata": {
    "id": "kdg4Uwmd9Hvs"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EUy4x8AYKhCd"
   },
   "source": [
    "References\n",
    "==========\n",
    "\n",
    "1. [Multi30K: Multilingual English-German Image Descriptions](https://arxiv.org/abs/1605.00459)\n",
    "2. Attention is all you need paper.\n",
    "    <https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf>\n",
    "3.  The annotated transformer.\n",
    "    <https://nlp.seas.harvard.edu/2018/04/03/attention.html#positional-encoding>\n",
    "4.  Blogpost transformers explained https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452\n",
    "5.  Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient estimation of word representations in vector space. <https://arxiv.org/abs/1301.3781>\n",
    "6.  Sutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to sequence learning with neural networks. <https://proceedings.neurips.cc/paper/2014/hash/a14ac55a4f27472c5d894ec1c3c743d2-Abstract.html>\n",
    "7.  Bahdanau, D., Cho, K., & Bengio, Y. (2014). Neural machine translation by jointly learning to alignand translate. <https://arxiv.org/abs/1409.0473>\n",
    "8.  He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. <https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html>\n",
    "9.  Ba, J. L., Kiros, J. R., & Hinton, G. E. (2016). Layer normalization. <https://arxiv.org/abs/1607.06450>\n",
    "10.  Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory <https://ieeexplore.ieee.org/abstract/document/6795963>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "toc_visible": true,
   "collapsed_sections": [
    "qVbaTo-r2SDT",
    "pS9EPDnI3MRS",
    "NI78Bpme4ghc",
    "DqyWcu97571S",
    "NIW6DvgT7bIl"
   ]
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
