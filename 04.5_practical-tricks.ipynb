{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9592383,"sourceType":"datasetVersion","datasetId":5850732}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Pitfalls of Deep Learning and Practical tricks for training","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.datasets as datasets\nfrom torchvision import transforms, io\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import DataLoader\nimport torch.optim as optim\nfrom PIL import Image\nimport copy","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.manual_seed(42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Models, classes and functions\nFor today, we will use several classes and models, that you can potentially re-use in assigments and your personal projects","metadata":{}},{"cell_type":"code","source":"# Simple MLP as described in Lecture 4\nclass Model(nn.Module):\n    def __init__(self, input_size, d_1, d_2, output_size):\n        super(Model, self).__init__()\n        self.fc1 = nn.Linear(input_size, d_1)\n        self.fc2 = nn.Linear(d_1, d_2)\n        self.fc3 = nn.Linear(d_2, output_size)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.fc2(x)\n        x = F.relu(x)\n        x = self.fc3(x)\n        return x\n\n# Class for generating noisy linear data, as described during Lecture 2\ndef create_dataset(sample_size=10, sigma=0.1, w_star=1, b_star = 1,\n                   x_range=(-1, 1), seed=0):\n    # Set the random state in numpy\n    torch.manual_seed(seed)\n    # Unpack the values in x_range\n    x_min, x_max = x_range\n    # Sample sample_size points from a uniform distribution\n    X = torch.rand(sample_size)\n    # Rescale between x_min and x_max \n    X = X * (x_max - x_min) + x_min\n    # Compute hat(y)\n    y_hat = X * w_star + b_star\n    # Compute y (Add Gaussian noise)\n    y = y_hat + torch.normal(torch.zeros(sample_size), sigma*torch.ones(sample_size))\n    return X, y\n\n# Simpler model\nclass SimpleModel(nn.Module):\n    def __init__(self, input_size, d_1, output_size):\n        super(SimpleModel, self).__init__()\n        self.fc1 = nn.Linear(input_size, d_1)\n        self.fc2 = nn.Linear(d_1, output_size)\n        \n    def forward(self, x):\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.fc2(x)\n        return x\n    \n# Model with regularization\nclass RegModel(nn.Module):\n    def __init__(self, input_size, d_1, d_2, output_size, p=0.2): \n        super(RegModel, self).__init__()\n        self.fc1 = nn.Linear(input_size, d_1)\n        self.BN = nn.BatchNorm1d(d_1)\n        self.fc2 = nn.Linear(d_1, d_2)\n        self.dropout = nn.Dropout(p)\n        self.fc3 = nn.Linear(d_2, output_size)\n\n             \n    def forward(self, x):\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.BN(x)\n        x = self.fc2(x)\n        x = F.relu(x)\n        x = self.dropout(x)\n        x = self.fc3(x)\n        return x\n    \n# Template class for Early Stopping\nclass EarlyStopping():\n    def __init__(self, patience=5, min_delta=0, restore_best_weights=True):\n        self.patience = patience\n        self.min_delta = min_delta # minimum improvement to reset patience\n        self.restore_best_weights = restore_best_weights\n        self.best_model = None\n        self.best_loss = None\n        self.counter = 0\n        self.status = \"\"\n        \n    def __call__(self, model, val_loss):\n        if self.best_loss is None:\n            self.best_loss = val_loss\n            self.best_model = copy.deepcopy(model)\n        elif self.best_loss - val_loss > self.min_delta:\n            self.best_loss = val_loss\n            self.counter = 0\n            self.best_model.load_state_dict(model.state_dict())\n        elif self.best_loss - val_loss < self.min_delta:\n            self.counter += 1\n            if self.counter >= self.patience:\n                self.status = f\"Stopped on {self.counter}\"\n                if self.restore_best_weights:\n                    model.load_state_dict(self.best_model.state_dict())\n                return True\n        self.status = f\"{self.counter}/{self.patience}\"\n        return False\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Overconfidence in Deep Learning\n\nLet's start with past week model, where we were able to achieve good accuracy values","metadata":{}},{"cell_type":"code","source":"# Datasets\ndataset_train = datasets.MNIST('.', train=True, download=True, transform=transforms.ToTensor()) # download = True just once\ndataset_test = datasets.MNIST('.', train=False, download=True, transform=transforms.ToTensor())\n\ndataset_validation, dataset_test = torch.utils.data.random_split(dataset_test, [0.5, 0.5])\n\n# Dataloader\nbatch_size = 64 # Reduce it in case you need it\n\ntrain_loader = DataLoader(dataset_train, batch_size=batch_size)\nvalidation_loader = DataLoader(dataset_validation, batch_size=len(dataset_validation))\ntest_loader = DataLoader(dataset_test, batch_size=len(dataset_test))\n\n# Hyperparameters\nd_1 = 100\nd_2 = 50\nmodel = Model(28*28, d_1, d_2, 10)\n\n# Hyperparameter!\nlearning_rate = 0.001\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\nloss_fn = nn.CrossEntropyLoss()\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'mps' \n    if torch.backends.mps.is_available() else 'cpu')\nmodel = model.to(DEVICE)\nprint(\"Working on\", DEVICE)\n\nn_epochs = 5\nfor epoch in range(n_epochs):\n    for data, target in train_loader:\n        # Set the model in training mode\n        model.train()\n        data, target = data.to(DEVICE), target.to(DEVICE)\n        # You have to flatten the data!\n        data = data.reshape(-1,28*28)\n        # Set the gradient to 0\n        optimizer.zero_grad()\n        # Make a prediction\n        output = model(data)\n        # Compute the loss function\n        loss = loss_fn(output, target)\n        # Backpropagation\n        loss.backward()\n        # Update parameters\n        optimizer.step()\n        \n    train_loss = loss.item()\n    # At the end of every epoch, check the validation loss value\n    with torch.no_grad():\n        model.eval()\n        for data, target in validation_loader: # Just one batch\n            data, target = data.to(DEVICE), target.to(DEVICE)\n             # You have to flatten the data!\n            data = data.reshape(-1,28*28)\n            # Make a prediction\n            output = model(data)\n            # Compute the loss function\n            validation_loss = loss_fn(output, target).item()\n            print(f\"Epoch {epoch + 1}: Train loss: {train_loss}, Validation loss {validation_loss}\")\n            \n    \n# Compute the accuracy on the test set\nwith torch.no_grad():\n    n_correct = 0\n    n_samples = 0\n    for data, target in test_loader:\n        data, target = data.to(DEVICE), target.to(DEVICE)\n        # You have to flatten the data!\n        data = data.reshape(-1,28*28)\n        outputs = model(data)\n        _, predicted = torch.max(outputs.data, 1)\n        n_samples += target.size(0)\n        n_correct += (predicted == target).sum().item()\n\n    acc = 100.0 * n_correct / n_samples\nprint(\"Accuracy on the test set:\", acc, \"%\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"What happens if we try to predict the picture of a Chicken?","metadata":{}},{"cell_type":"code","source":"# Load the image\nimage = Image.open('/kaggle/input/practical-tricks-data/chicken_28x28.png')\n\ndisplay(image)\n\ntransform = transforms.Compose([\n    transforms.PILToTensor()\n])\n\nX = transform(image).float().flatten()\nprint(X.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we want to predict the class of the chicken","metadata":{}},{"cell_type":"markdown","source":"We note that our model is overconfidence that is a 8 for no particular reason","metadata":{}},{"cell_type":"code","source":"X = X.to(DEVICE)\noutput = model(X) # TODO \nsm = torch.nn.Softmax(dim=0)\nprobabilities = sm(output) \nprint(\"With probability\", 100 * max(probabilities).item(), \"% is a \", torch.argmax(probabilities).item())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Overfitting\n\nIn this part of the notebook, we fake an Overfitting case to see what's happening for the loss function. To do so, we generate some random noisy data froma linear function and try to learn them in a non-linear way. We will see that the learnt curve is non linear.","metadata":{}},{"cell_type":"code","source":"# Parameters we want to learn\nw_star = 2\nb_star = 1\n\nnum_samples_train = 20\nnum_samples_validation = 10\n\n# Set the seed\nseed_train = 0\nseed_validation = 1\n\n# Set a value of noise (=sigma)\nsigma = 1.3\n\n# Define x_range\nx = (-2, 2)\n\n# Generate train data\nX_train, y_train = create_dataset(\n    sample_size=num_samples_train, sigma=sigma, w_star=w_star,\n    b_star = b_star, x_range=x, seed=seed_train)\n\n# Generate the validation data form the same distribution but with a different seed\nX_val, y_val = create_dataset(\n    sample_size=num_samples_validation, sigma=sigma, w_star=w_star,\n    b_star = b_star, x_range=x, seed=seed_validation)\n\n# Reshape data\nX_train = X_train.reshape(-1, 1)\ny_train = y_train.reshape(-1, 1)\nX_val = X_val.reshape(-1, 1)\ny_val = y_val.reshape(-1, 1)\n\n\n# Define a model\nmodel = SimpleModel(1, 400, 1)\n\n# Loss function\nloss_fn = nn.MSELoss() \n\n# Learning rate\nlearning_rate = 0.01\n\n# Optimizer\noptimizer = optim.SGD(model.parameters(), lr=learning_rate)\n\n# Training loop\ntrain_loss_vals = []\nval_loss_vals = []\nn_steps = 5000 # Number of updates of the gradient\nfor step in range(n_steps):\n    model.train() # Set the model in training mode\n    # Set the gradient to 0\n    optimizer.zero_grad() # Or model.zero_grad()\n    # Compute the output of the model\n    y_hat = model(X_train)\n    # Compute the loss\n    loss = loss_fn(y_hat, y_train)\n    # Compute the gradient\n    loss.backward()\n    # Update the parameters\n    optimizer.step()\n    # *** Evaluation ***\n    # Here we do things that do not contribute to the gradient computation\n    model.eval() # Set the model in evaluation mode\n    with torch.no_grad(): #\n        # Compute the output of the model\n        y_hat_val = model(X_val)\n        # Compute the loss\n        loss_val = loss_fn(y_hat_val, y_val)\n        if step % 50 == 0:\n            # Print the losses\n            print(\"Step:\", step, \"Train loss: \", loss.item(), \"- Loss eval:\", loss_val.item())\n    train_loss_vals.append(loss.item())\n    val_loss_vals.append(loss_val.item())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plot the loss functions together... do you notice something strange?","metadata":{}},{"cell_type":"code","source":"plt.figure()\nplt.plot(range(n_steps),train_loss_vals)\nplt.plot(range(n_steps),val_loss_vals)\nplt.legend([\"Tr. loss\", \"Val. loss\"])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Plot the training data alongside with the learned function. Note that we are trying to resemble the data and not generalize them!","metadata":{}},{"cell_type":"code","source":"plt.figure()\nplt.plot(X_train, y_train, 'o')\nx_range = torch.linspace(-2, 2, steps=1000)\nx_range = x_range.reshape(-1, 1)\nplt.plot(x_range, model(x_range).detach().numpy())\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training with Dropout and Batch Normalization\n\nIn this section, we train a model having both Dropout and Batch normalization on the MNIST dataset. Eventually, we will compare its performances on both the test set and the chicken image. Observe that, in this case, the training is slower and you actually need GPUs","metadata":{}},{"cell_type":"code","source":"# Datasets\ndataset_train = datasets.MNIST('.', train=True, download=False, transform=transforms.ToTensor())\ndataset_test = datasets.MNIST('.', train=False, download=False, transform=transforms.ToTensor())\n\ndataset_validation, dataset_test = torch.utils.data.random_split(dataset_test, [0.5, 0.5])\n\n# Dataloader\nbatch_size = 64 # Reduce it in case you need it\n\ntrain_loader = DataLoader(dataset_train, batch_size=batch_size)\nvalidation_loader = DataLoader(dataset_validation, batch_size=len(dataset_validation))\ntest_loader = DataLoader(dataset_test, batch_size=len(dataset_test))\n\n# Hyperparameters\nd_1 = 100\nd_2 = 50\nmodel = RegModel(28*28, d_1, d_2, 10)\n\n# Hyperparameter!\nlearning_rate = 0.001\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\nloss_fn = nn.CrossEntropyLoss()\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'mps' \n    if torch.backends.mps.is_available() else 'cpu')\nmodel = model.to(DEVICE)\nprint(\"Working on\", DEVICE)\n\nn_epochs = 5\nfor epoch in range(n_epochs):\n    for data, target in train_loader:\n        # Set the model in training mode\n        model.train()\n        data, target = data.to(DEVICE), target.to(DEVICE)\n        # You have to flatten the data!\n        data = data.reshape(-1,28*28)\n        # Set the gradient to 0\n        optimizer.zero_grad()\n        # Make a prediction\n        output = model(data)\n        # Compute the loss function\n        loss = loss_fn(output, target)\n        # Backpropagation\n        loss.backward()\n        # Update parameters\n        optimizer.step()\n        \n    train_loss = loss.item()\n    # At the end of every epoch, check the validation loss value\n    with torch.no_grad():\n        model.eval()\n        for data, target in validation_loader: # Just one batch\n            data, target = data.to(DEVICE), target.to(DEVICE)\n             # You have to flatten the data!\n            data = data.reshape(-1,28*28)\n            # Make a prediction\n            output = model(data)\n            # Compute the loss function\n            validation_loss = loss_fn(output, target).item()\n            print(f\"Epoch {epoch + 1}: Train loss: {train_loss}, Validation loss {validation_loss}\")\n            \n    \n# Compute the accuracy on the test set\nmodel.eval()\nwith torch.no_grad():\n    n_correct = 0\n    n_samples = 0\n    for data, target in test_loader:\n        data, target = data.to(DEVICE), target.to(DEVICE)\n        # You have to flatten the data!\n        data = data.reshape(-1,28*28)\n        outputs = model(data)\n        _, predicted = torch.max(outputs.data, 1)\n        n_samples += target.size(0)\n        n_correct += (predicted == target).sum().item()\n\n    acc = 100.0 * n_correct / n_samples\nprint(\"Accuracy on the test set:\", acc, \"%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Early stopping\n\nIn this section, we will retrain our \"Simple\" model for several epochs on the MNIST dataset. We will observe the usage of the Early stopping class","metadata":{}},{"cell_type":"code","source":"# Datasets\ndataset_train = datasets.MNIST('.', train=True, download=False, transform=transforms.ToTensor())\ndataset_test = datasets.MNIST('.', train=False, download=False, transform=transforms.ToTensor())\n\ndataset_validation, dataset_test = torch.utils.data.random_split(dataset_test, [0.5, 0.5])\n\n# Dataloader\nbatch_size = 64 # Reduce it in case you need it\n\ntrain_loader = DataLoader(dataset_train, batch_size=batch_size)\nvalidation_loader = DataLoader(dataset_validation, batch_size=len(dataset_validation))\ntest_loader = DataLoader(dataset_test, batch_size=len(dataset_test))\n\n# Hyperparameters\nd_1 = 100\nd_2 = 50\nmodel = Model(28*28, d_1, d_2, 10)\n\n# Hyperparameter!\nlearning_rate = 0.001\n\n# Optimizer, loss, Early stop\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\nloss_fn = nn.CrossEntropyLoss()\nearly_stop = EarlyStopping(patience=2, min_delta = 0.0001)\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'mps' \n    if torch.backends.mps.is_available() else 'cpu')\nmodel = model.to(DEVICE)\nprint(\"Working on\", DEVICE)\n\nn_epochs = 30\nfor epoch in range(n_epochs):\n    for data, target in train_loader:\n        # Set the model in training mode\n        model.train()\n        data, target = data.to(DEVICE), target.to(DEVICE)\n        # You have to flatten the data!\n        data = data.reshape(-1,28*28)\n        # Set the gradient to 0\n        optimizer.zero_grad()\n        # Make a prediction\n        output = model(data)\n        # Compute the loss function\n        loss = loss_fn(output, target)\n        # Backpropagation\n        loss.backward()\n        # Update parameters\n        optimizer.step()\n        \n    train_loss = loss.item()\n    # At the end of every epoch, check the validation loss value\n    model.eval()\n    with torch.no_grad():\n        for data, target in validation_loader: # Just one batch\n            data, target = data.to(DEVICE), target.to(DEVICE)\n             # You have to flatten the data!\n            data = data.reshape(-1,28*28)\n            # Make a prediction\n            output = model(data)\n            # Compute the loss function\n            validation_loss = loss_fn(output, target).item()\n            print(f\"Epoch {epoch + 1}: Train loss: {train_loss}, Validation loss {validation_loss}\")\n    \n    if early_stop(model, validation_loss):\n        print(f\"Stopped trained at Epoch {epoch + 1}\")\n        break\n\n            \n    \n# Compute the accuracy on the test set\nmodel.eval()\nwith torch.no_grad():\n    n_correct = 0\n    n_samples = 0\n    for data, target in test_loader:\n        data, target = data.to(DEVICE), target.to(DEVICE)\n        # You have to flatten the data!\n        data = data.reshape(-1,28*28)\n        outputs = model(data)\n        _, predicted = torch.max(outputs.data, 1)\n        n_samples += target.size(0)\n        n_correct += (predicted == target).sum().item()\n\n    acc = 100.0 * n_correct / n_samples\nprint(\"Accuracy on the test set:\", acc, \"%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## $L_1, L_2$ penalties in the training loop\n\nIn this section, we see how to implement an $L_1, L_2$ penalty in the training loop. Note that there is a flag, `reg` that you can either set equal to 1 or 2. It asks you the value of $\\lambda$ to be used for the training","metadata":{}},{"cell_type":"code","source":"# Datasets\ndataset_train = datasets.MNIST('.', train=True, download=False, transform=transforms.ToTensor())\ndataset_test = datasets.MNIST('.', train=False, download=False, transform=transforms.ToTensor())\n\ndataset_validation, dataset_test = torch.utils.data.random_split(dataset_test, [0.5, 0.5])\n\n# Dataloader\nbatch_size = 64 # Reduce it in case you need it\n\ntrain_loader = DataLoader(dataset_train, batch_size=batch_size)\nvalidation_loader = DataLoader(dataset_validation, batch_size=len(dataset_validation))\ntest_loader = DataLoader(dataset_test, batch_size=len(dataset_test))\n\n# Hyperparameters\nd_1 = 100\nd_2 = 50\nmodel = Model(28*28, d_1, d_2, 10)\n\n# Hyperparameter!\nlearning_rate = 0.01\n\nr = input(\"Insert the regularization method, 1 or 2:\")\nr = int(r)\nl = input(\"Insert lambda value\")\nl = float(l)\nif r == 2:\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=l)\nelif r == 1:\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.)\nelse:\n    raise ValueError(\"Not an L1/L2 regularization\")\n\nloss_fn = nn.CrossEntropyLoss()\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'mps' \n    if torch.backends.mps.is_available() else 'cpu')\nmodel = model.to(DEVICE)\nprint(\"Working on\", DEVICE)\n\nn_epochs = 5\n\nfor epoch in range(n_epochs):\n    for data, target in train_loader:\n        # Set the model in training mode\n        model.train()\n        data, target = data.to(DEVICE), target.to(DEVICE)\n        # You have to flatten the data!\n        data = data.reshape(-1,28*28)\n        # Set the gradient to 0\n        optimizer.zero_grad()\n        # Make a prediction\n        output = model(data)\n        # Compute the loss function\n        loss = loss_fn(output, target)\n        \n        # In case of L1, add the penalty\n        if r == 1:\n            l1_norm = sum(p.abs().sum() for p in model.parameters())\n            loss += l * l1_norm\n        \n        # Backpropagation\n        loss.backward()\n        # Update parameters\n        optimizer.step()\n    print(\"Last train loss of the epoch:\", loss.item())\n            \n# Compute the accuracy on the test set\nmodel.eval()\nwith torch.no_grad():\n    n_correct = 0\n    n_samples = 0\n    for data, target in test_loader:\n        data, target = data.to(DEVICE), target.to(DEVICE)\n        # You have to flatten the data!\n        data = data.reshape(-1,28*28)\n        outputs = model(data)\n        _, predicted = torch.max(outputs.data, 1)\n        n_samples += target.size(0)\n        n_correct += (predicted == target).sum().item()\n\n    acc = 100.0 * n_correct / n_samples\nprint(\"Accuracy on the test set:\", acc, \"%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training with accumulated gradent\n\nIn this section, we show with the most simple model we have and we training using accumulated gradient\n","metadata":{}},{"cell_type":"code","source":"# Datasets\ndataset_train = datasets.MNIST('.', train=True, download=False, transform=transforms.ToTensor())\ndataset_test = datasets.MNIST('.', train=False, download=False, transform=transforms.ToTensor())\n\ndataset_validation, dataset_test = torch.utils.data.random_split(dataset_test, [0.5, 0.5])\n\n# Dataloader\nbatch_size = 64 # Reduce it in case you need it\naccum_iter = 4\n\ntrain_loader = DataLoader(dataset_train, batch_size=batch_size)\nvalidation_loader = DataLoader(dataset_validation, batch_size=len(dataset_validation))\ntest_loader = DataLoader(dataset_test, batch_size=len(dataset_test))\n\n# Hyperparameters\nd_1 = 100\nd_2 = 50\nmodel = Model(28*28, d_1, d_2, 10)\n\n# Hyperparameter!\nlearning_rate = 0.001\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\nloss_fn = nn.CrossEntropyLoss()\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'mps' \n    if torch.backends.mps.is_available() else 'cpu')\nmodel = model.to(DEVICE)\nprint(\"Working on\", DEVICE)\n\nn_epochs = 5\nfor epoch in range(n_epochs):\n    for batch_idx, (data, target) in enumerate(train_loader):\n        # Set the model in training mode\n        model.train()\n        # Forward pass\n        data, target = data.to(DEVICE), target.to(DEVICE)\n        # You have to flatten the data!\n        data = data.reshape(-1,28*28)\n        # Make a prediction\n        output = model(data)\n        # Compute the loss function\n        loss = loss_fn(output, target)\n        \n        # Normalize the loss to account for batch normalization \n        # e.g every batch contributes with 1/accum_iter on the whole loss\n        loss = loss / accum_iter\n        \n        # Backpropagation\n        loss.backward()\n        \n        if (batch_idx + 1) % accum_iter == 0: # Every accum_iter iterations....\n            # Update parameters\n            optimizer.step()\n            # Set the gradient to 0\n            optimizer.zero_grad()\n        \n    train_loss = loss.item()\n    # At the end of every epoch, check the validation loss value\n    with torch.no_grad():\n        model.eval()\n        for data, target in validation_loader: # Just one batch\n            data, target = data.to(DEVICE), target.to(DEVICE)\n             # You have to flatten the data!\n            data = data.reshape(-1,28*28)\n            # Make a prediction\n            output = model(data)\n            # Compute the loss function\n            validation_loss = loss_fn(output, target).item()\n            print(f\"Epoch {epoch + 1}: Train loss: {train_loss}, Validation loss {validation_loss}\")\n            \n    \n# Compute the accuracy on the test set\nwith torch.no_grad():\n    n_correct = 0\n    n_samples = 0\n    for data, target in test_loader:\n        data, target = data.to(DEVICE), target.to(DEVICE)\n        # You have to flatten the data!\n        data = data.reshape(-1,28*28)\n        outputs = model(data)\n        _, predicted = torch.max(outputs.data, 1)\n        n_samples += target.size(0)\n        n_correct += (predicted == target).sum().item()\n\n    acc = 100.0 * n_correct / n_samples\nprint(\"Accuracy on the test set:\", acc, \"%\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training with a scheduler\n\nIn this section, we show the usage of a sceduler to reduce the learning rate when the validation loss does not decrease that much","metadata":{}},{"cell_type":"code","source":"# Datasets\ndataset_train = datasets.MNIST('.', train=True, download=True, transform=transforms.ToTensor()) # download = True just once\ndataset_test = datasets.MNIST('.', train=False, download=True, transform=transforms.ToTensor())\n\ndataset_validation, dataset_test = torch.utils.data.random_split(dataset_test, [0.5, 0.5])\n\n# Dataloader\nbatch_size = 64 # Reduce it in case you need it\n\ntrain_loader = DataLoader(dataset_train, batch_size=batch_size)\nvalidation_loader = DataLoader(dataset_validation, batch_size=len(dataset_validation))\ntest_loader = DataLoader(dataset_test, batch_size=len(dataset_test))\n\n# Hyperparameters\nd_1 = 100\nd_2 = 50\nmodel = Model(28*28, d_1, d_2, 10)\n\n# Hyperparameter!\nlearning_rate = 0.001\n\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\nloss_fn = nn.CrossEntropyLoss()\nscheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'mps' \n    if torch.backends.mps.is_available() else 'cpu')\nmodel = model.to(DEVICE)\nprint(\"Working on\", DEVICE)\n\nn_epochs = 10\nprint(\"Starting learning rate:\", learning_rate)\nfor epoch in range(n_epochs):\n    for data, target in train_loader:\n        # Set the model in training mode\n        model.train()\n        data, target = data.to(DEVICE), target.to(DEVICE)\n        # You have to flatten the data!\n        data = data.reshape(-1,28*28)\n        # Set the gradient to 0\n        optimizer.zero_grad()\n        # Make a prediction\n        output = model(data)\n        # Compute the loss function\n        loss = loss_fn(output, target)\n        # Backpropagation\n        loss.backward()\n        # Update parameters\n        optimizer.step()\n        \n    train_loss = loss.item()\n    # At the end of every epoch, check the validation loss value\n    with torch.no_grad():\n        model.eval()\n        for data, target in validation_loader: # Just one batch\n            data, target = data.to(DEVICE), target.to(DEVICE)\n             # You have to flatten the data!\n            data = data.reshape(-1,28*28)\n            # Make a prediction\n            output = model(data)\n            validation_loss = loss_fn(output, target)\n            print(f\"Epoch {epoch + 1}: Train loss: {train_loss}, Validation loss {validation_loss}\")\n            scheduler.step()\n            print(\"New learning rate:\", round(scheduler.get_last_lr()[0], 6))\n            \n    \n# Compute the accuracy on the test set\nwith torch.no_grad():\n    n_correct = 0\n    n_samples = 0\n    for data, target in test_loader:\n        data, target = data.to(DEVICE), target.to(DEVICE)\n        # You have to flatten the data!\n        data = data.reshape(-1,28*28)\n        outputs = model(data)\n        _, predicted = torch.max(outputs.data, 1)\n        n_samples += target.size(0)\n        n_correct += (predicted == target).sum().item()\n\n    acc = 100.0 * n_correct / n_samples\nprint(\"Accuracy on the test set:\", acc, \"%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}