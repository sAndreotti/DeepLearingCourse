{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1400d04be3274704",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "## Authors: E. Vercesi; A. Dei Rossi, G. Dominici, S. Huber\n",
    "\n",
    "In this exercise session you are going to learn the basics of PyTorch. \n",
    "PyTorch is a Python library for scientific computing (as much as NumPy), but which can additionally run on GPUs. \n",
    "Hence, this is the computing library of choice for Deep Learning applications. \n",
    "PyTorch is developed by Meta. You might have also heard of its main competitor TensorFlow (Google). Although both have basically the same functionalities, in this course we would like you to stick to Pytorch.\n",
    "If you haven't done Exercise 1 on NumPy yet, we highly encourage to do it first: NumPy and PyTorch offer a vast overlap of functionalities, so understanding NumPy first is going to boost greatly your understanding of PyTorch.\n",
    "To begin with, make sure you have installed it. "
   ]
  },
  {
   "cell_type": "code",
   "id": "85fa3c9d5375899a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T08:38:35.977246Z",
     "start_time": "2024-10-02T08:38:35.972950Z"
    }
   },
   "source": [
    "import torch  # If you see errors, use conda or pip to install torch in your virtual environment.\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(42)  # manual seed is to ensure repeatability of random numbers. "
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1116a1fd0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "id": "3b4e6da339cf2d72",
   "metadata": {},
   "source": [
    "**Question (for fun):** Why the seed is often [42](https://www.youtube.com/watch?v=aboZctrHfK8)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166656a7be8fb629",
   "metadata": {},
   "source": [
    "## Create tensors\n",
    "\n",
    "Tensors are like numpy arrays, but they can live in the GPU.\n",
    "\n",
    "1. Create a tensor out of a Python list [1, 2, 3]\n",
    "2. Create a tensor out of a NumPy array [[2, 3, 4], [4, 3, 2]] (see method [`.from_numpy()`](https://pytorch.org/docs/stable/generated/torch.from_numpy.html))\n",
    "3. Convert the tensor of point 2 back to a NumPy array. (see method [`.numpy()`](https://pytorch.org/docs/stable/generated/torch.Tensor.numpy.html))"
   ]
  },
  {
   "cell_type": "code",
   "id": "6628425e43a4729a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T08:38:36.000249Z",
     "start_time": "2024-10-02T08:38:35.996902Z"
    }
   },
   "source": [
    "## 1: create a tensor out of a Python list\n",
    "tens = torch.Tensor([1,2,3]) \n",
    "print(tens)\n",
    "\n",
    "## 2: create a tensor out of a NumPy array\n",
    "a = np.array([[2,3,4],[4,3,2]])\n",
    "t = torch.from_numpy(a)\n",
    "print(t)\n",
    "\n",
    "## 3: Convert the tensor back to NumPy.\n",
    "print(torch.Tensor.numpy(t))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "tensor([[2, 3, 4],\n",
      "        [4, 3, 2]])\n",
      "[[2 3 4]\n",
      " [4 3 2]]\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "id": "d6e04eaff1b59988",
   "metadata": {},
   "source": [
    "Check the `.dtype` attribute of the above created tensors. Create a tensor of size 3 with values [1, 2, 3] but forcing the dtype to be float64."
   ]
  },
  {
   "cell_type": "code",
   "id": "701958d3e03ea301",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T08:38:36.029715Z",
     "start_time": "2024-10-02T08:38:36.027344Z"
    }
   },
   "source": [
    "## 1: create [1, 2, 3] with dtype float64\n",
    "tens = torch.tensor([1,2,3], dtype=torch.float64)\n",
    "print(tens)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "id": "a49bf6b0cbecf456",
   "metadata": {},
   "source": [
    "PyTorch also offers some more advanced functions that can be used to create well known matrices:\n",
    "\n",
    "1. Create an identity matrix of size (5, 5) (see [`torch.eye()`](https://pytorch.org/docs/stable/generated/torch.eye.html)).\n",
    "2. Create a matrix of all zeros of size (3, 4) (see [`torch.zeros()`](https://pytorch.org/docs/stable/generated/torch.zeros.html).\n",
    "3. Create a matrix of all ones of size (2, 3) (see [`torch.ones()`](https://pytorch.org/docs/stable/generated/torch.ones.html).\n",
    "4. Given a tensor of size (3, 2) of your choice, create a matrix of the same size (3, 2) filled with ones (equivalently zeros) (see [`torch.zeros_like()`](https://pytorch.org/docs/stable/generated/torch.zeros_like.html)\n",
    "5. Create a matrix of size (3, 4) filled with numbers from 0 to 11 inclusive (same as in NumPy). Try both [`torch.arange()`](https://pytorch.org/docs/stable/generated/torch.arange.html) and [`torch.linspace()`](https://pytorch.org/docs/stable/generated/torch.linspace.html)."
   ]
  },
  {
   "cell_type": "code",
   "id": "58eba4bbd65d8a27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T08:38:36.053048Z",
     "start_time": "2024-10-02T08:38:36.049154Z"
    }
   },
   "source": [
    "## 1:\n",
    "identity = torch.eye(5,5)\n",
    "print(identity)\n",
    "\n",
    "## 2:\n",
    "all_zeros = torch.zeros(3, 4)\n",
    "print(all_zeros)\n",
    "\n",
    "## 3:\n",
    "all_ones = torch.ones(2,3)\n",
    "print(all_ones)\n",
    "\n",
    "## 4:\n",
    "tens = torch.tensor([[1,2,3],[4,5,6]])\n",
    "zeros_like = torch.zeros_like(tens)\n",
    "print(zeros_like)\n",
    "\n",
    "## 5:\n",
    "## torch.arange()\n",
    "tens = torch.arange(start=0, end=12, out=torch.zeros(3, 4))\n",
    "print(tens)\n",
    "\n",
    "## torch.linspace()\n",
    "tens2 = torch.linspace(start=0, end=12, steps=12, out=torch.zeros(3, 4))\n",
    "print(tens2)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1.]])\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0]])\n",
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.]])\n",
      "tensor([[ 0.0000,  1.0909,  2.1818,  3.2727],\n",
      "        [ 4.3636,  5.4545,  6.5455,  7.6364],\n",
      "        [ 8.7273,  9.8182, 10.9091, 12.0000]])\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "id": "4e58877dfd4627fc",
   "metadata": {},
   "source": [
    "### Random arrays\n",
    "\n",
    "As in NumPy, you have a big choice of random distributions to sample you arrays from.\n",
    "Try to do the same random arrays you tried to do in NumPy in Exercise 1:\n",
    "1) Create a random tensor of size 4 of uniform floating point numbers in the interval [0, 1). (see [`torch.rand`](https://pytorch.org/docs/stable/generated/torch.rand.html))\n",
    "2) Create a random tensor of size (3, 2) of uniform floating point numbers in the interval [0, 5). (hint: generate numbers in the interval [0, 1) and scale them up by 5).\n",
    "3) Create a random tensor of size (2, 1, 2) of integers in the interval [10, 20]. (see [`torch.randint`](https://pytorch.org/docs/stable/generated/torch.randint.html), caraful with border conditions!)\n",
    "4) Create a random tensor of size 10 over the normal distribution, mean 3 and std dev 2. (see [`torch.normal`](https://pytorch.org/docs/stable/generated/torch.normal.html))"
   ]
  },
  {
   "cell_type": "code",
   "id": "108b3718b4ff25d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T08:38:36.079822Z",
     "start_time": "2024-10-02T08:38:36.076612Z"
    }
   },
   "source": [
    "## 1:\n",
    "tens = torch.rand(4)\n",
    "print(tens)\n",
    "\n",
    "## 2:\n",
    "tens = torch.rand(3, 2)\n",
    "print(tens+4)\n",
    "\n",
    "## 3:\n",
    "tens = torch.randint(low=10, high=21, size=(2,1,2))\n",
    "print(tens)\n",
    "\n",
    "## 4:\n",
    "tens = torch.normal(mean=3, std=2, size=(10,))\n",
    "print(tens)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8823, 0.9150, 0.3829, 0.9593])\n",
      "tensor([[4.3904, 4.6009],\n",
      "        [4.2566, 4.7936],\n",
      "        [4.9408, 4.1332]])\n",
      "tensor([[[11, 20]],\n",
      "\n",
      "        [[13, 14]]])\n",
      "tensor([3.7531, 2.6384, 3.7861, 3.8654, 0.2746, 5.7129, 4.3376, 1.5846, 2.3466,\n",
      "        2.4424])\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "id": "9298efd452bfa973",
   "metadata": {},
   "source": [
    "## Device (GPU vs CPU)\n",
    "\n",
    "In this section we will learn how do computation using the GPU instead of the CPU: notice that this is the reason why in Deep Learning applications PyTorch is used over NumPy.\n",
    "\n",
    "By default, tensors are accessed by the CPU. You can check it easily using the [`.device()`](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device) method.\n",
    "1) Create an identity matrix of size (4, 4) and access its device attribute."
   ]
  },
  {
   "cell_type": "code",
   "id": "e3a205fdbd7b65c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T08:38:39.583708Z",
     "start_time": "2024-10-02T08:38:39.566204Z"
    }
   },
   "source": [
    "## 1: see .device of a matrix\n",
    "\n",
    "v = torch.eye(4,4)  # create a tensor\n",
    "print(torch.cuda.current_device())"
   ],
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[32], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m## 1: see .device of a matrix\u001B[39;00m\n\u001B[1;32m      3\u001B[0m v \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39meye(\u001B[38;5;241m4\u001B[39m,\u001B[38;5;241m4\u001B[39m)  \u001B[38;5;66;03m# create a tensor\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28mprint\u001B[39m(torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mcurrent_device())\n",
      "File \u001B[0;32m~/miniconda3/envs/DeepLearning/lib/python3.12/site-packages/torch/cuda/__init__.py:878\u001B[0m, in \u001B[0;36mcurrent_device\u001B[0;34m()\u001B[0m\n\u001B[1;32m    876\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcurrent_device\u001B[39m() \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mint\u001B[39m:\n\u001B[1;32m    877\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Return the index of a currently selected device.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 878\u001B[0m     _lazy_init()\n\u001B[1;32m    879\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_cuda_getDevice()\n",
      "File \u001B[0;32m~/miniconda3/envs/DeepLearning/lib/python3.12/site-packages/torch/cuda/__init__.py:305\u001B[0m, in \u001B[0;36m_lazy_init\u001B[0;34m()\u001B[0m\n\u001B[1;32m    300\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    301\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    302\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmultiprocessing, you must use the \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mspawn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m start method\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    303\u001B[0m     )\n\u001B[1;32m    304\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(torch\u001B[38;5;241m.\u001B[39m_C, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_cuda_getDeviceCount\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 305\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTorch not compiled with CUDA enabled\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    306\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _cudart \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    307\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\n\u001B[1;32m    308\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    309\u001B[0m     )\n",
      "\u001B[0;31mAssertionError\u001B[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "id": "bdd04377c451cc9e",
   "metadata": {},
   "source": [
    "Hence, every time we want to use the GPU, we need to explicitly move the tensors to the desired device. Careful here: your laptop doesn't necessarily have a dedicated GPU. And, even if it has one, it might not be compatible with CUDA (the NVIDIA interface that allows computations to be performed on the GPU).\n",
    "\n",
    "You can check if CUDA is available on your machine by simply using [`cuda.is_available()`](https://pytorch.org/docs/stable/generated/torch.cuda.is_available.html)."
   ]
  },
  {
   "cell_type": "code",
   "id": "40633cabf32ec826",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T08:38:44.738047Z",
     "start_time": "2024-10-02T08:38:44.735394Z"
    }
   },
   "source": [
    "torch.cuda.is_available()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "id": "b848fb3e3e1d87a3",
   "metadata": {},
   "source": [
    "If the above returns False, it could be either because you didn't install correctly CUDA, or because you laptop doesn't have a GPU compatible with it. \n",
    "If you have a macbook with Apple Silicon processors, you can still use the device `mps`:"
   ]
  },
  {
   "cell_type": "code",
   "id": "34009eeefef2b8a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T08:38:47.207888Z",
     "start_time": "2024-10-02T08:38:47.204896Z"
    }
   },
   "source": [
    "# For mac M1/2/3 users\n",
    "torch.backends.mps.is_available()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "id": "578906289e3a356",
   "metadata": {},
   "source": [
    "We can set the device either to these three options:\n",
    "- `cuda` (if you have a NVIDIA graphics card). Might be `cuda:0` etc if you have more than one.\n",
    "- `mps` (if you have a MacBook with M1/2/3 processor)\n",
    "- `cpu` otherwise\n",
    "\n",
    "If your laptop doesn't have any of the above-mentioned devices apart from the CPU, you can use Google Colab's or Kaggle's notebooks: they offer free hours of GPU per week (they count the hours the kernel is running, not if you are actually using the notebook. Remember to shut it down when you don't use it!!!)"
   ]
  },
  {
   "cell_type": "code",
   "id": "2dc0d82333ea4030",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T08:38:49.307604Z",
     "start_time": "2024-10-02T08:38:49.304135Z"
    }
   },
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "device"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "id": "e2a7ceb325529382",
   "metadata": {},
   "source": [
    "Finally, move the tensor `v` you created earlier to the most convenient device. Use function [`.to`](https://pytorch.org/docs/stable/generated/torch.Tensor.to.html). Careful: is it an in-place method? Check that the device is indeed correct."
   ]
  },
  {
   "cell_type": "code",
   "id": "9c331d995b0e7c26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T08:38:50.967539Z",
     "start_time": "2024-10-02T08:38:50.964740Z"
    }
   },
   "source": [
    "# Move vector v to the correct device. Check it is indeed on the desired device.\n",
    "v = v.to(device)"
   ],
   "outputs": [],
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "id": "9932e553cde60656",
   "metadata": {},
   "source": [
    "You can also create a tensor and send it directly to the correct device. \n",
    "1. Create a tensor of ones of size (3, 3) and specify in its constructor the `device` attribute. Check that, indeed, the tensor has been initialized with the correct device."
   ]
  },
  {
   "cell_type": "code",
   "id": "2f9247113e7f4981",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T08:38:52.613320Z",
     "start_time": "2024-10-02T08:38:52.608014Z"
    }
   },
   "source": [
    "## 1: Create a tensor and initialize it to the correct device.\n",
    "print(torch.tensor([1,2,3], device=device))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3], device='mps:0')\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "id": "dad27eb333bb8200",
   "metadata": {},
   "source": [
    "Later on, we will compute empirically how much faster are GPUs than CPUs for computing large calculations.\n",
    "\n",
    "If you are using Kaggle platform for your projects (we recommend you to do that), you have at your disposal 30h/week of free GPUs: in order to activate it, you need to open a notebook, go to settings -> accelerator and you can select a GPU from there. If the GPU options are non-clickable, it is because you have to verify your account using your phone number. Go to home -> your picture (top right border) -> settings -> phone verification. Before the options become actually clickable you will need to wait a few minutes (<5').\n",
    "\n",
    "If you are using Google Colab (also recommended), you can activate GPUs by opening a notebook -> top-right arrow pointing downward -> change runtime type -> select something which is not `CPU`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d972460fb05e0a",
   "metadata": {},
   "source": [
    "## Working with tensors' dimensions\n",
    "\n",
    "In this section we will learn how to manipulate tensor's dimensions. Notice that they are extremely similar to NumPy methods: hence, if you have done exercise 1, this section should be quite straightforward.\n",
    "\n",
    "### Access elements and slicing \n",
    "\n",
    "Create an identity matrix of size (4, 4) and access \n",
    "1. The element in position [0, 0]\n",
    "2. The last element\n",
    "3. Element in position [2, 3]\n",
    "Check that the returned elements are what you expect."
   ]
  },
  {
   "cell_type": "code",
   "id": "60958f0e86811752",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T08:38:55.464782Z",
     "start_time": "2024-10-02T08:38:55.460546Z"
    }
   },
   "source": [
    "# Create the identity matrix\n",
    "identity = torch.eye(4,4)\n",
    "\n",
    "## 1: access element in [0, 0]\n",
    "print(identity[0][0])\n",
    "\n",
    "## 2: access element in [3, 3]\n",
    "print(identity[-1][-1])\n",
    "\n",
    "## 3: access element in [2, 3]\n",
    "print(identity[2][3])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "tensor(1.)\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "id": "48e067adf3b30b97",
   "metadata": {},
   "source": [
    "### Slicing\n",
    "\n",
    "1. Create a random tensor of size (3, 4) of integers in the interval [5, 10].\n",
    "2. Print the second row.\n",
    "3. Print the third column.\n",
    "4. Print the sub-matrix spanning from the second to the third row, from the second to the third column."
   ]
  },
  {
   "cell_type": "code",
   "id": "dddd2e53d3e6ac70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T08:38:57.855030Z",
     "start_time": "2024-10-02T08:38:57.851191Z"
    }
   },
   "source": [
    "## 1: create a random tensor of size [3, 4].\n",
    "tens = torch.randint(low=5, high=11, size=(3,4))\n",
    "print(tens)\n",
    "\n",
    "## 2: print the second row.\n",
    "print(tens[1,:])\n",
    "\n",
    "## 3: print the third column.\n",
    "print(tens[:,2])\n",
    "\n",
    "## 4: sub-matrix\n",
    "print(tens[1:3,1:3])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10,  9, 10,  7],\n",
      "        [ 9,  5,  7,  5],\n",
      "        [ 6,  8,  8, 10]])\n",
      "tensor([9, 5, 7, 5])\n",
      "tensor([10,  7,  8])\n",
      "tensor([[5, 7],\n",
      "        [8, 8]])\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "id": "d46a439f",
   "metadata": {},
   "source": [
    "### Access tensors' dimensions\n",
    "\n",
    "1. Create a tensor $v$ of size (3, 4, 2, 4, 1) of random floats in [0, 1)\n",
    "2. Print its shape. You can use both `.shape` and `.size()`, try them both.\n",
    "3. Print its third dimension's size (2 in our example). Check `.size()` function.\n",
    "4. Print the number of dimensions of our vector (5 in our example). Check `.ndim`."
   ]
  },
  {
   "cell_type": "code",
   "id": "602276de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T08:39:00.254586Z",
     "start_time": "2024-10-02T08:39:00.251527Z"
    }
   },
   "source": [
    "## 1: create a random tensor v of size (3, 4, 2, 4, 1).\n",
    "tens = torch.rand(size=(3,4,3,4,2))\n",
    "\n",
    "## 2: print v's shape using .shape and .size().\n",
    "print(f'Shape: {tens.shape}')\n",
    "print(f'Size: {tens.size()}')\n",
    "\n",
    "## 3: print the size of the third dimension of v.\n",
    "print(f'3rd size: {tens.size(2)}')\n",
    "\n",
    "## 4: print the number of dimensions of v.\n",
    "print(f'Dimensions: {tens.ndim}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: torch.Size([3, 4, 3, 4, 2])\n",
      "Size: torch.Size([3, 4, 3, 4, 2])\n",
      "3rd size: 3\n",
      "Dimensions: 5\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "id": "b3beec5d76fa615b",
   "metadata": {},
   "source": [
    "## Permute dimensions\n",
    "\n",
    "You can invert the order of the dimensions of a tensor. Create a random tensor of integers in the interval [0, 10) of size (2, 3, 4) and permute its dimensions so that the final size is (4, 2, 3). See [`torch.permute`](https://pytorch.org/docs/stable/generated/torch.permute.html)"
   ]
  },
  {
   "cell_type": "code",
   "id": "c1e286754f2b797f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T08:39:02.497406Z",
     "start_time": "2024-10-02T08:39:02.494448Z"
    }
   },
   "source": [
    "# Create a random tensor. Check its shape (2, 3, 4)\n",
    "tens = torch.rand(size=(2,3,4))\n",
    "print(tens.size())\n",
    "\n",
    "# Permute its dimensions. Check its shape (4, 2, 3)\n",
    "print(tens.permute(2,0,1).shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n",
      "torch.Size([4, 2, 3])\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "markdown",
   "id": "3d684fd09d5a4bb8",
   "metadata": {},
   "source": [
    "## Squeeze/unsqueeze\n",
    "\n",
    "If you want increase the number of dimensions of your vector (similar to `np.newaxis`, this might turn useful in the context of broadcasting), you can use [`torch.unsqueeze`](https://pytorch.org/docs/stable/generated/torch.unsqueeze.html). If you want to reduce the number of dimensions of your vector by dropping dimensions of size 1 you can use [`torch.squeeze`](https://pytorch.org/docs/stable/generated/torch.squeeze.html) instead.\n",
    "\n",
    "1. Create a random tensor uniform in [0, 1) of size (2, 2). Insert a new dimension so that the final shape is (2, 1, 2)\n",
    "2. Add a dimension to the tensor of point 1, so that the final shape is (2, 1, 2, 1). Try to use negative indices as the argument of `torch.unsqueeze()`.\n",
    "3. Turn the tensor back to its original shape (2, 2) by using `torch.squeeze()`."
   ]
  },
  {
   "cell_type": "code",
   "id": "b671e74f22171ec8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T08:39:04.821918Z",
     "start_time": "2024-10-02T08:39:04.818617Z"
    }
   },
   "source": [
    "## 1: Create a tensor of size (2, 2). Unsqueeze it so that its final shape is (2, 1, 2)\n",
    "tens = torch.rand(size=(2,2))\n",
    "tens = tens.unsqueeze(1)\n",
    "print(tens.shape)\n",
    "\n",
    "## 2: Add an additional dimension to the tensor so that its shape is (2, 1, 2, 1). Use negative indices\n",
    "tens = tens.unsqueeze(-1)\n",
    "print(tens.shape)\n",
    "\n",
    "## 3: Turn the tensor back to shape (2, 2)\n",
    "tens = tens.squeeze(-1)\n",
    "tens = tens.squeeze(1)\n",
    "print(tens.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 2])\n",
      "torch.Size([2, 1, 2, 1])\n",
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "cell_type": "markdown",
   "id": "e93229b6fa5c1e07",
   "metadata": {},
   "source": [
    "## Concatenate and stack\n",
    "\n",
    "If you have two tensors of compatible sizes, you can merge them into a unique tensor along one of their axes.\n",
    "In order to get some intuition, think about having 2 2-dimensional tensors of size (3, 4). You can merge them along the first axis and get the final shape be (6, 4), or you can merge them along the second axis and get the final shape to be (3, 8), or you can go in 3D stacking one over the other (along the z-axis) and get a shape of (2, 3, 4). \n",
    "\n",
    "This is precisely what [`torch.concat`](https://pytorch.org/docs/stable/generated/torch.cat.html#torch.cat) (also called `.cat`) and [`torch.stack`](https://pytorch.org/docs/stable/generated/torch.stack) do. \n",
    "You should already be familiar with NumPy `axis` attribute. In PyTorch it is called `dim`.\n",
    "\n",
    "1. Concat $v$ and $w$ along the first dimension. Check that the final shape is (6, 4).\n",
    "2. Concat $v$ and $w$ along the second dimension. Check that the final shape is (3, 8).\n",
    "3. Concat $v$ and $w$ along a new dimension. Check that the final shape is (2, 3, 4)."
   ]
  },
  {
   "cell_type": "code",
   "id": "e9399001d3b3efa4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T08:39:07.323969Z",
     "start_time": "2024-10-02T08:39:07.320498Z"
    }
   },
   "source": [
    "v = torch.randint(0, 10, (3,4))\n",
    "w = torch.randint(0, 10, (3,4))\n",
    "\n",
    "## 1: concat along first dimension.\n",
    "print(torch.cat((v, w), dim=0).shape)\n",
    "\n",
    "## 2: concat along second dimension.\n",
    "print(torch.cat((v, w), dim=1).shape)\n",
    "\n",
    "\n",
    "## 3: concat along new dimension.\n",
    "print(torch.stack((v,w)).shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 4])\n",
      "torch.Size([3, 8])\n",
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "cell_type": "markdown",
   "id": "7695c739ef39b4bb",
   "metadata": {},
   "source": [
    "## Broadcasting\n",
    "\n",
    "Same as in NumPy, also PyTorch tensors allow [broadcasting](https://pytorch.org/docs/stable/notes/broadcasting.html).\n",
    "When performing element-wise operations (like sums) on two tensors of mismatching sizes, the smaller tensor can adapt to the size of the larger tensor in case these simple rules apply:\n",
    "\n",
    "- Each tensor has at least one dimension.\n",
    "- When iterating over the dimension sizes, starting at the trailing (right-most) dimension, the dimension sizes must either be equal, one of them is 1, or one of them does not exist.\n",
    "\n",
    "Let us see an example:\n",
    "\n",
    "Assume you have $v = [[1, 2, 3], [4, 5, 6]]$ shape (2, 3) and $w=[3, 2, 1]$ shape (3). If we want to perform $v + w$ (element by element sum), it is clear that the dimensions don't match, but with the help of broadcasting we can still do it: $w$ is simply enlarged to reach size (2, 3) by copying itself on the first axis twice. Then, it is possible to perform element by element sum $v+w$.\n",
    "\n",
    "Let's put broadcasting in practice:\n",
    "\n",
    "1. Perform the above described example $v+w$ using tensors, check that the result size is (2, 3) and that numbers add up.\n",
    "2. $r = [[1, 2], [3, 4], [5, 6]]$ and $l=[1, 2, 3]$. Compute $r + l$. It should raise errors. Why?\n",
    "3. Adjust the size of $l$ in example 2 so that the sum works. What size should $l$ have in order for broadcasting to work on $r + l$?\n",
    "4. Create random integers tensors $s$ of size (2, 1, 3, 1) and $t$ of size (1, 3, 1, 3). Does broadcasting work here in order to compute $s+t$? In case it does, predict the final shape of the result. \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "id": "25c1ff851e016878",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T08:42:53.862372Z",
     "start_time": "2024-10-02T08:42:53.858408Z"
    }
   },
   "source": [
    "## 1: compute v + w\n",
    "v=torch.tensor([[1,2,3],[4,5,6]])\n",
    "w=torch.tensor([3,2,1])\n",
    "print((v+w).shape)\n",
    "print((v+w))\n",
    "\n",
    "## 2: compute r + l. It doesn't work, why?\n",
    "r=torch.tensor([[1,2],[3,4],[5,6]])\n",
    "l=torch.tensor([1,2,3])\n",
    "# Error all dimensions mismatch\n",
    "\n",
    "## 3: adjust the size of l, and compute r + l\n",
    "l=l.unsqueeze(1)\n",
    "print((r+l).shape)\n",
    "print(r+l)\n",
    "# Adding a dimension to l to make the left most equal,\n",
    "# the right one will be expanded\n",
    "\n",
    "## 4: compute s + t\n",
    "s=torch.rand(size=(2,1,3,1))\n",
    "t=torch.rand(size=(1,3,1,3))\n",
    "print('Predicted shape: 2,3,3,3')\n",
    "print((s+t).shape)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "tensor([[4, 4, 4],\n",
      "        [7, 7, 7]])\n",
      "torch.Size([3, 2])\n",
      "tensor([[2, 3],\n",
      "        [5, 6],\n",
      "        [8, 9]])\n",
      "Predicted shape: 2,3,3,3\n",
      "torch.Size([2, 3, 3, 3])\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "cell_type": "markdown",
   "id": "9e5ea76fcf50b553",
   "metadata": {},
   "source": [
    "## PyTorch functions\n",
    "\n",
    "In this section we are going to learn the basic functions of PyTorch.\n",
    "\n",
    "### Mean, min, max, sum ...\n",
    "\n",
    "These functions are quite self-explanatory, and they work the same way as in NumPy. The only detail we ought to pay attention to is the axis along we want to perform the function (in NumPy it was called `axis`, in PyTorch `dim`).\n",
    "\n",
    "Create a random tensor $v$ of ints of size (3, 2, 4) and print it.\n",
    "\n",
    "In order to be sure you have understood what is going on, always try to predict the result and then check that your prediction is wrong/correct.\n",
    "\n",
    "1. Compute the min value in the entire tensor.\n",
    "2. Compute the max value along axis 0.\n",
    "3. Compute the min along axis 1.\n",
    "4. Multi-dimensional axes: take the sum over axes (0, 1). \n"
   ]
  },
  {
   "cell_type": "code",
   "id": "6212dd7b00b8bc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T08:43:33.876834Z",
     "start_time": "2024-10-02T08:43:33.873557Z"
    }
   },
   "source": [
    "# Create v of shape (3, 2, 4)\n",
    "v=torch.rand(size=(3,2,4))\n",
    "print(v)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.8377, 0.5398, 0.5226, 0.3769],\n",
      "         [0.0472, 0.0299, 0.2610, 0.2458]],\n",
      "\n",
      "        [[0.6558, 0.3544, 0.3044, 0.9767],\n",
      "         [0.6742, 0.8565, 0.2579, 0.2958]],\n",
      "\n",
      "        [[0.6838, 0.1669, 0.1731, 0.4759],\n",
      "         [0.3171, 0.1252, 0.7966, 0.9021]]])\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "id": "de343d48896069b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T08:47:16.763572Z",
     "start_time": "2024-10-02T08:47:16.759125Z"
    }
   },
   "source": [
    "## 1: Compute the min value of v.\n",
    "print(torch.min(v))\n",
    "\n",
    "## 2: Compute the max along axis 0.\n",
    "print(torch.max(v, dim=0))\n",
    "\n",
    "## 3: Compute the min along axis 1.\n",
    "print(torch.min(v, dim=1))\n",
    "\n",
    "## 4: Compute the sum over axes (0, 1)\n",
    "print(torch.sum(v, dim=(0,1)))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0299)\n",
      "torch.return_types.max(\n",
      "values=tensor([[0.8377, 0.5398, 0.5226, 0.9767],\n",
      "        [0.6742, 0.8565, 0.7966, 0.9021]]),\n",
      "indices=tensor([[0, 0, 0, 1],\n",
      "        [1, 1, 2, 2]]))\n",
      "torch.return_types.min(\n",
      "values=tensor([[0.0472, 0.0299, 0.2610, 0.2458],\n",
      "        [0.6558, 0.3544, 0.2579, 0.2958],\n",
      "        [0.3171, 0.1252, 0.1731, 0.4759]]),\n",
      "indices=tensor([[1, 1, 1, 1],\n",
      "        [0, 0, 1, 1],\n",
      "        [1, 1, 0, 0]]))\n",
      "tensor([3.2157, 2.0726, 2.3156, 3.2732])\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "cell_type": "markdown",
   "id": "a68f8062d06c1ada",
   "metadata": {},
   "source": [
    "### dot, matmul, transpose, *\n",
    "\n",
    "Unlike NumPy, Torch has a stricter policy on these operands:\n",
    "\n",
    "- `*`: is the Hadamard product, element-wise product.\n",
    "- `dot`: only used to compute the dot product of two 1-dimensional tensors. Remember how confusing the dot product between multi-dimension NumPy vectors is (see Exercise 1)? PyTorch avoids this issue by simply forbidding the dimension of the input tensors to be greater than 1.\n",
    "- `matmul`: or its alias `@` computes the matrix product. Can be used for larger than 2-dimensional tensors (it applies broadcasting, as much as in NumPy). Notice that the complexity of multiplying two $n\\times n$ matrices is $O(n^3)$. We are taking advantage of its relatively high time-complexity in order to show how much faster are GPUs wrt CPUs.\n",
    "\n",
    "1. Create two random integer tensors $A$ and $B$ of compatible(?) sizes and compute their Hadamard product (element by element product). Try these sizes (predict whether they work or not):\n",
    "    - $A$ size (3, 4), $B$ size (3, 4).\n",
    "    - $A$ size (3, 4), $B$ size (4, 4).\n",
    "    - $A$ size (3, 4), $B$ size (1, 4).\n",
    "2. Create two random 1-dimensional tensors $v, w$ and compute their dot product. If you can use multiple ways to compute it, check that indeed they return the same value.\n",
    "3. Create $C$ of size (3, 4) and $D$ of size (4, 3). Compute the matrix product. Are the sizes compatible?\n",
    "4. Create $E$ of size (3, 3) and $F$ of size (4, 3). Compute the matrix product. Are the sizes compatible? If not, use the transpose operator to adjust the dimensions of one of the two matrices and compute the matrix product.\n",
    "\n",
    "\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T08:56:17.627416Z",
     "start_time": "2024-10-02T08:56:17.621437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## 1: Create A, B and perform hadamard product\n",
    "# A (3,4), B (3,4)\n",
    "a=torch.rand(size=(3,4))\n",
    "b=torch.rand(size=(3,4))\n",
    "print(a*b)\n",
    "\n",
    "# A (3,4), B (4,4)\n",
    "a=torch.rand(size=(3,4))\n",
    "b=torch.rand(size=(4,4))\n",
    "# Not work, bigger dimensions\n",
    "\n",
    "# A (3,4), B (1,4)\n",
    "a=torch.rand(size=(3,4))\n",
    "b=torch.rand(size=(1,4))\n",
    "print(a*b)\n",
    "# Work, smaller dim is broadcasted\n",
    "\n",
    "## 2: Create 1-dimensional tensors v, w and compute their dot product.\n",
    "v=torch.rand(size=(3,))\n",
    "w=torch.rand(size=(3,))\n",
    "print(v*w) # element-wise\n",
    "print(torch.dot(v,w)) # equal to matmul\n",
    "print(torch.matmul(v,w)) # equal to dot\n",
    "print()\n",
    "\n",
    "## 3: Compute matrix product of C and D.\n",
    "c=torch.rand(size=(3,4))\n",
    "d=torch.rand(size=(4,3))\n",
    "print(torch.matmul(c,d))\n",
    "\n",
    "## 4: adjust dimensions using .T, and compute the matrix product E @ F\n",
    "e=torch.rand(size=(3,3))\n",
    "f=torch.rand(size=(4,3))\n",
    "print(torch.matmul(e,f.T))\n"
   ],
   "id": "d3b71901207acd48",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7078, 0.1388, 0.0141, 0.0495],\n",
      "        [0.3971, 0.7466, 0.1627, 0.2145],\n",
      "        [0.4738, 0.1941, 0.0821, 0.4177]])\n",
      "tensor([[0.1567, 0.2391, 0.0105, 0.2605],\n",
      "        [0.2786, 0.1061, 0.1162, 0.3285],\n",
      "        [0.4849, 0.1626, 0.0314, 0.5266]])\n",
      "tensor([0.0014, 0.1091, 0.1050])\n",
      "tensor(0.2155)\n",
      "tensor(0.2155)\n",
      "\n",
      "tensor([[0.9410, 1.5292, 2.0286],\n",
      "        [0.3608, 0.6621, 0.9526],\n",
      "        [0.9816, 1.2992, 1.9389]])\n",
      "tensor([[0.5395, 0.5800, 0.8887, 1.0214],\n",
      "        [0.4686, 0.4916, 0.6768, 0.8854],\n",
      "        [0.2077, 0.1947, 0.2746, 0.3073]])\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "cell_type": "markdown",
   "id": "ac52ff8888dba981",
   "metadata": {},
   "source": [
    "Now, we try to prove empirically that GPUs are actually faster than CPUs at doing large calculations.\n",
    "\n",
    "Create a large tensor $G, H$ both of size (15000, 15000). Take their matrix product and measure how long it takes (use `%%time` cell magic notebook function)."
   ]
  },
  {
   "cell_type": "code",
   "id": "941d22401d6f340c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T08:56:35.283574Z",
     "start_time": "2024-10-02T08:56:34.042819Z"
    }
   },
   "source": [
    "# Create E and F\n",
    "G = torch.rand(15000, 15000)\n",
    "H = torch.rand(15000, 15000)"
   ],
   "outputs": [],
   "execution_count": 67
  },
  {
   "cell_type": "code",
   "id": "50f9bad11da746e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T08:56:43.689545Z",
     "start_time": "2024-10-02T08:56:40.597069Z"
    }
   },
   "source": [
    "%%time\n",
    "G @ H"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.71 s, sys: 387 ms, total: 6.09 s\n",
      "Wall time: 3.09 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[3785.3940, 3736.2322, 3745.4905,  ..., 3756.3398, 3790.1528,\n",
       "         3786.3767],\n",
       "        [3773.0840, 3734.7131, 3755.4543,  ..., 3776.8916, 3823.2778,\n",
       "         3786.1375],\n",
       "        [3809.5737, 3760.7314, 3766.8491,  ..., 3791.2261, 3828.6909,\n",
       "         3807.8506],\n",
       "        ...,\n",
       "        [3806.4031, 3771.0356, 3763.5601,  ..., 3778.9180, 3803.5864,\n",
       "         3786.3508],\n",
       "        [3786.4512, 3750.9412, 3757.9817,  ..., 3773.0549, 3811.6399,\n",
       "         3810.8909],\n",
       "        [3796.5281, 3740.4800, 3771.4990,  ..., 3812.6238, 3819.9070,\n",
       "         3784.4470]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 68
  },
  {
   "cell_type": "markdown",
   "id": "b53eec9dfd6c023b",
   "metadata": {},
   "source": [
    "Move $E$ and $F$ to the more convenient device at your disposal (different from CPU, if possible), and compute the same matrix product."
   ]
  },
  {
   "cell_type": "code",
   "id": "727c2e81c3e93874",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T08:57:26.392049Z",
     "start_time": "2024-10-02T08:57:26.389519Z"
    }
   },
   "source": [
    "# Move the tensors to GPU in another cell, so that the time is not counted.\n",
    "G = G.to(device)\n",
    "H = H.to(device)"
   ],
   "outputs": [],
   "execution_count": 71
  },
  {
   "cell_type": "code",
   "id": "ac5312cb12b5f605",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T08:57:30.348078Z",
     "start_time": "2024-10-02T08:57:27.657658Z"
    }
   },
   "source": [
    "%%time\n",
    "G @ H"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 708 μs, sys: 685 μs, total: 1.39 ms\n",
      "Wall time: 605 μs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[3785.3940, 3736.2322, 3745.4905,  ..., 3756.3362, 3790.1523,\n",
       "         3786.3767],\n",
       "        [3773.0840, 3734.7131, 3755.4543,  ..., 3776.8896, 3823.2729,\n",
       "         3786.1440],\n",
       "        [3809.5737, 3760.7314, 3766.8491,  ..., 3791.2261, 3828.6880,\n",
       "         3807.8479],\n",
       "        ...,\n",
       "        [3806.4031, 3771.0356, 3763.5601,  ..., 3778.9099, 3803.5769,\n",
       "         3786.3477],\n",
       "        [3786.4512, 3750.9412, 3757.9817,  ..., 3773.0645, 3811.6387,\n",
       "         3810.8950],\n",
       "        [3796.5281, 3740.4800, 3771.4990,  ..., 3812.6321, 3819.9019,\n",
       "         3784.4470]], device='mps:0')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 72
  },
  {
   "cell_type": "markdown",
   "id": "6a8f563c9873d5c",
   "metadata": {},
   "source": [
    "Side note: on my laptop (MacBook) I noticed a performance improvement by $\\approx\\times 10$. On Kaggle the performance improvement is much larger (from >20'' to <<1').\n",
    "When you have done this task, you might want to shut down your notebook and start from the cells below since resource usage might be quite demanding. Also, if you are using Kaggle, you might consider shutting down the GPU, since the bottom cells can be done with CPU only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bf73d907408b34",
   "metadata": {},
   "source": [
    "### PyTorch functionals\n",
    "\n",
    "As you will learn by attending this class, one of the key features of neural networks are their non-linear functions.\n",
    "PyTorch has already implemented a great amount of them in the package `torch.nn.functionals`.\n",
    "Create a random tensor $A$ of size (3, 2) and apply to it:\n",
    "\n",
    "1. [ReLu](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)\n",
    "2. [Tanh](https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html)\n",
    "3. [Sigmoid](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html)\n",
    "4. [Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html) (it requires an axis: pick axis 1, predict the output shape).\n",
    "\n",
    "If you are not familiar with them don't worry, you will learn in the remainder of the course what these functions are used for."
   ]
  },
  {
   "cell_type": "code",
   "id": "893065c65c067750",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T09:09:50.364877Z",
     "start_time": "2024-10-02T09:09:50.362622Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn.functional as f"
   ],
   "outputs": [],
   "execution_count": 90
  },
  {
   "cell_type": "code",
   "id": "96717c9a8f603bab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T09:10:40.140069Z",
     "start_time": "2024-10-02T09:10:40.131963Z"
    }
   },
   "source": [
    "# Create A\n",
    "A = torch.rand(3, 2) * 10 - 5\n",
    "#print(A)\n",
    "\n",
    "## 1: apply F.relu\n",
    "print(f.relu(A))\n",
    "print()\n",
    "\n",
    "## 2: apply F.tanh\n",
    "print(f.tanh(A))\n",
    "print()\n",
    "\n",
    "## 3: apply F.sigmoid\n",
    "print(f.sigmoid(A))\n",
    "print()\n",
    "\n",
    "## 4: apply F.softmax with dim=1\n",
    "print(f.softmax(A, dim=1))\n",
    "print()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 4.9915],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0000, 3.7730]])\n",
      "\n",
      "tensor([[-0.9993,  0.9999],\n",
      "        [-0.9696, -0.9083],\n",
      "        [-0.8759,  0.9989]])\n",
      "\n",
      "tensor([[0.0188, 0.9933],\n",
      "        [0.1105, 0.1798],\n",
      "        [0.2046, 0.9775]])\n",
      "\n",
      "tensor([[1.3052e-04, 9.9987e-01],\n",
      "        [3.6165e-01, 6.3835e-01],\n",
      "        [5.8757e-03, 9.9412e-01]])\n",
      "\n"
     ]
    }
   ],
   "execution_count": 93
  },
  {
   "cell_type": "markdown",
   "id": "c9176dfb298f4b9e",
   "metadata": {},
   "source": [
    "## Gradients\n",
    "\n",
    "One of the useful features of PyTorch is that it is possible to compute automatically the gradient of functions. \n",
    "As you will see, the gradient of a function is one of the key ingredients of the backpropagation algorithm, used to train neural nets.\n",
    "\n",
    "Assume we have tensor $x = [2], y = [2]$. We have $z = 2x^2 + 3y = [14]$.\n",
    "\n",
    "We know that $\\frac{\\delta z}{\\delta x} = 4x$, $\\frac{\\delta z}{\\delta y} = 3$. Since we are evaluating the point $x=2, y=2$, we get that the gradient is (8, 3). The gradients are going to be stored in $x.grad$ and $y.grad$ if we specify the option `requires_grad=True`. We can let PyTorch compute the gradients by invoking `z.backward()`. Check that indeed `x.grad` and `y.grad` hold the desired values.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "2e3e254da08cb300",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T09:24:11.224596Z",
     "start_time": "2024-10-02T09:24:11.212414Z"
    }
   },
   "source": [
    "x = torch.tensor([2], dtype=torch.float64, requires_grad=True)\n",
    "y = torch.tensor([2], dtype=torch.float64, requires_grad=True)\n",
    "z = 2 * x*x + 3 * y\n",
    "z.backward()\n",
    "print(x.grad)\n",
    "print(y.grad)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8.], dtype=torch.float64)\n",
      "tensor([3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "execution_count": 94
  },
  {
   "cell_type": "markdown",
   "id": "d0f3648bc606d705",
   "metadata": {},
   "source": [
    "1. Create tensors $s = [1]$ and $t = [1]$, define a new variable $w = 5s + 6$ and compute their gradient. What is the gradient associated to $t$? (Notice that $w$ does not depend on $t$). \n",
    "2. What happens if I try to define an integer tensor with `requires_grad=True`?\n",
    "3. What happens if I call `numpy()` on a tensor that has `requires_grad=True`?"
   ]
  },
  {
   "cell_type": "code",
   "id": "54c046fde0ee306e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T09:27:38.081196Z",
     "start_time": "2024-10-02T09:27:38.078381Z"
    }
   },
   "source": [
    "## 1: gradient of t for w = 5s + 6.\n",
    "s = torch.tensor([1], dtype=torch.float64, requires_grad=True)\n",
    "t = torch.tensor([1], dtype=torch.float64, requires_grad=True)\n",
    "w = 5*s+6\n",
    "w.backward()\n",
    "print(t.grad) # none, w not depend on t\n",
    "\n",
    "## 2: integer tensor with requires_grad.\n",
    "# only floating point can have gradient\n",
    "\n",
    "## 3: compute .numpy() of a tensor with requires_grad.\n",
    "# t = torch.tensor([1], dtype=torch.float64, requires_grad=True).numpy() can't call directly\n",
    "# must use tensor.detach().numpy()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "execution_count": 98
  },
  {
   "cell_type": "markdown",
   "id": "3834bc4e",
   "metadata": {},
   "source": [
    "In this last section we point out a very important feature of gradients, namely that they are *cumulative*! In order to see what does that mean, let's see in practice the example that was given in class:\n",
    "\n",
    "1. Create tensors $x=[2], y=[3]$ (with flag `requires_grad=True`).\n",
    "2. Compute $z = x * x + y$ and perform the backward pass.\n",
    "3. Check that the gradients are as expected: $\\frac{\\delta z}{\\delta x}=2x=4$, $\\frac{\\delta z}{\\delta y} = 1$.\n",
    "4. Compute $g = xy + 3x$ and perform che backward pass.\n",
    "5. Check out the gradients: $\\frac{\\delta g}{\\delta x}=y + 3=6$, $\\frac{\\delta g}{\\delta y} = x = 2$.\n",
    "6. Do you see the expected value? Can you explain why? (hint: gradients are *cumulative*).\n",
    "7. In order to fix this potential issue, use `x/y.grad.zero_()` in between the computation of $z$ and $g$. Do you observe the expected gradient now?"
   ]
  },
  {
   "cell_type": "code",
   "id": "c0e33d42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T09:33:50.127204Z",
     "start_time": "2024-10-02T09:33:50.120989Z"
    }
   },
   "source": [
    "## 1: create x, y.\n",
    "x = torch.tensor([2], dtype=torch.float64, requires_grad=True)\n",
    "y = torch.tensor([3], dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "## 2: compute z.\n",
    "z = x*x+y\n",
    "z.backward()\n",
    "\n",
    "## 3: check out gradients of x, y.\n",
    "print(f'x gradient: {x.grad}')\n",
    "print(f'y gradient: {y.grad}')\n",
    "\n",
    "## 4: compute g.\n",
    "g = x*y+3*x\n",
    "g.backward()\n",
    "\n",
    "## 5: check out gradients of x, y\n",
    "print(f'x gradient: {x.grad}')\n",
    "print(f'y gradient: {y.grad}')\n",
    "\n",
    "## 6: Gradients are comulative so they are not 0 \n",
    "# when I compute them with g\n",
    "print()\n",
    "\n",
    "## 7: Repeat 1-5 using torch.zero_grad()\n",
    "x.grad.zero_()\n",
    "y.grad.zero_()\n",
    "\n",
    "# compute z.\n",
    "z = x*x+y\n",
    "z.backward()\n",
    "# check out gradients of x, y.\n",
    "print(f'x gradient: {x.grad}')\n",
    "print(f'y gradient: {y.grad}')\n",
    "\n",
    "x.grad.zero_()\n",
    "y.grad.zero_()\n",
    "\n",
    "# compute g.\n",
    "g = x*y+3*x\n",
    "g.backward()\n",
    "# check out gradients of x, y\n",
    "print(f'x gradient: {x.grad}')\n",
    "print(f'y gradient: {y.grad}')\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x gradient: tensor([4.], dtype=torch.float64)\n",
      "y gradient: tensor([1.], dtype=torch.float64)\n",
      "x gradient: tensor([10.], dtype=torch.float64)\n",
      "y gradient: tensor([3.], dtype=torch.float64)\n",
      "\n",
      "x gradient: tensor([4.], dtype=torch.float64)\n",
      "y gradient: tensor([1.], dtype=torch.float64)\n",
      "x gradient: tensor([6.], dtype=torch.float64)\n",
      "y gradient: tensor([2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "execution_count": 101
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
